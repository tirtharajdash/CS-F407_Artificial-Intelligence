{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e964b21",
   "metadata": {},
   "source": [
    "### LLM + RAG demonstration: Neurosymbolic AI and their application in Science\n",
    "\n",
    "February 2026\n",
    "\n",
    "This notebook demonstrates Retrieval-Augmented Generation (RAG) over a corpus of neurosymbolic AI and drug discovery papers, co-authored by [Tirtharaj Dash](https://tirtharajdash.github.io). This notebook may contain bugs, if so please let TD know. Also, this notebook is a part of the AI course (CS F407) only designed for demonstration.\n",
    "\n",
    "**We have used the following models:**\n",
    "- Encoder: `BAAI/bge-small-en-v1.5` (dense retrieval, 384-dim)\n",
    "- Generator: `Qwen/Qwen2.5-3B-Instruct` (instruction-tuned, ~6 GB RAM)\n",
    "\n",
    "To use a lighter model (CPU-only): set `GEN_MODEL = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"`  \n",
    "To use a heavier model (better quality): set `GEN_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"`\n",
    "\n",
    "**Libraries and packages to be installed before running this notebook**:\n",
    "```python\n",
    "#BE CAREFUL!!! IT MIGHT MESS YOUR ENV. I SUGGEST CREATE A NEW ENV and RUN THIS NOTEBOOK.\\\n",
    "#!pip install -q pymupdf sentence-transformers faiss-cpu transformers accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eddba6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers directory : /home/tirtharaj/dash/CS-F407_Artificial-Intelligence/labs/Lab3/Dash_NeuroSym_Paps\n",
      "PDFs found       : 9\n",
      "Embed model      : BAAI/bge-small-en-v1.5\n",
      "Generator model  : Qwen/Qwen2.5-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PAPERS_DIR   = Path(\"Dash_NeuroSym_Paps\")   # folder with our papers' PDFs\n",
    "INDEX_FILE   = Path(\"neurosym.index\")       # FAISS index (saved after first build)\n",
    "CORPUS_FILE  = Path(\"neurosym_corpus.pkl\")  # chunk text + metadata\n",
    "\n",
    "CHUNK_WORDS  = 200    # words per chunk\n",
    "OVERLAP      = 40     # word overlap between consecutive chunks\n",
    "\n",
    "EMBED_MODEL  = \"BAAI/bge-small-en-v1.5\"  # 384-dim, fast, good quality\n",
    "TOP_K        = 4                         # chunks to retrieve per query\n",
    "\n",
    "GEN_MODEL    = \"Qwen/Qwen2.5-3B-Instruct\"   # good enough model\n",
    "MAX_NEW_TOK  = 350\n",
    "\n",
    "print(f\"Papers directory : {PAPERS_DIR.resolve()}\")\n",
    "print(f\"PDFs found       : {len(list(PAPERS_DIR.glob('*.pdf')))}\")\n",
    "print(f\"Embed model      : {EMBED_MODEL}\")\n",
    "print(f\"Generator model  : {GEN_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f33a76",
   "metadata": {},
   "source": [
    "**We need to first ingest PDFs and build FAISS index.**\n",
    "\n",
    "Each PDF is split into overlapping word-level chunks. Each chunk is embedded by\n",
    "`BAAI/bge-small-en-v1.5` and stored in a FAISS inner-product index (equivalent\n",
    "to cosine similarity after L2 normalisation).\n",
    "\n",
    "The index is saved to disk so this step runs only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc70005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing index ...\n",
      "Index loaded: 1094 vectors, dim=384\n"
     ]
    }
   ],
   "source": [
    "import pymupdf as fitz                                  # PyMuPDF\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def extract_chunks(pdf_path: Path, chunk_words: int, overlap: int):\n",
    "    \"\"\"Extract overlapping word-level chunks from a PDF.\"\"\"\n",
    "    doc  = fitz.open(str(pdf_path))\n",
    "    text = \" \".join(page.get_text() for page in doc)\n",
    "    doc.close()\n",
    "\n",
    "    words  = text.split()\n",
    "    stride = chunk_words - overlap\n",
    "    chunks, meta = [], []\n",
    "    for i in range(0, len(words), stride):\n",
    "        chunk = \" \".join(words[i : i + chunk_words])\n",
    "        if len(chunk.strip()) < 60:          # skip near-empty trailing chunks\n",
    "            continue\n",
    "        chunks.append(chunk)\n",
    "        meta.append({\"source\": pdf_path.name, \"word_offset\": i})\n",
    "    return chunks, meta\n",
    "\n",
    "\n",
    "def build_index(papers_dir: Path, chunk_words: int, overlap: int,\n",
    "                embed_model_name: str, index_file: Path, corpus_file: Path):\n",
    "    pdfs = sorted(papers_dir.glob(\"*.pdf\"))\n",
    "    assert pdfs, f\"No PDFs found in {papers_dir}\"\n",
    "\n",
    "    print(f\"Ingesting {len(pdfs)} PDFs ...\")\n",
    "    corpus, metadata = [], []\n",
    "    for pdf in tqdm(pdfs, desc=\"Parsing PDFs\"):\n",
    "        c, m = extract_chunks(pdf, chunk_words, overlap)\n",
    "        corpus.extend(c)\n",
    "        metadata.extend(m)\n",
    "    print(f\"Total chunks: {len(corpus)}\")\n",
    "\n",
    "    print(f\"\\nEncoding with {embed_model_name} ...\")\n",
    "    encoder    = SentenceTransformer(embed_model_name)\n",
    "    embeddings = encoder.encode(\n",
    "        corpus, batch_size=64, show_progress_bar=True,\n",
    "        convert_to_numpy=True, normalize_embeddings=True  # L2-normalise -> cosine via IP\n",
    "    )\n",
    "\n",
    "    dim   = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)   # inner product on unit vectors = cosine similarity\n",
    "    index.add(embeddings)\n",
    "\n",
    "    faiss.write_index(index, str(index_file))\n",
    "    with open(corpus_file, \"wb\") as f:\n",
    "        pickle.dump((corpus, metadata), f)\n",
    "\n",
    "    print(f\"\\nIndex saved to  : {index_file}\")\n",
    "    print(f\"Corpus saved to : {corpus_file}\")\n",
    "    return index, corpus, metadata, encoder\n",
    "\n",
    "\n",
    "# Build or load\n",
    "if INDEX_FILE.exists() and CORPUS_FILE.exists():\n",
    "    print(\"Loading existing index ...\")\n",
    "    index = faiss.read_index(str(INDEX_FILE))\n",
    "    with open(CORPUS_FILE, \"rb\") as f:\n",
    "        corpus, metadata = pickle.load(f)\n",
    "    encoder = SentenceTransformer(EMBED_MODEL)\n",
    "    print(f\"Index loaded: {index.ntotal} vectors, dim={index.d}\")\n",
    "else:\n",
    "    index, corpus, metadata, encoder = build_index(\n",
    "        PAPERS_DIR, CHUNK_WORDS, OVERLAP, EMBED_MODEL, INDEX_FILE, CORPUS_FILE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e6fbd3",
   "metadata": {},
   "source": [
    "**Retrieval function**\n",
    "\n",
    "Given a query string $q$, compute $\\mathbf{e}_q = f_\\phi(q)$ and return the\n",
    "top-$k$ chunks by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44827629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4 chunks for sanity check:\n",
      "  [0.809] 2510.23379v1.pdf\n",
      "  [0.790] 27751-Article Text-31805-1-2-20240324.pdf\n",
      "  [0.787] 2510.23379v1.pdf\n",
      "  [0.784] 2510.23379v1.pdf\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query: str, k: int = TOP_K):\n",
    "    \"\"\"\n",
    "    Embed query and return top-k chunks with scores and source filenames.\n",
    "    Returns a list of dicts: {chunk, source, score}\n",
    "    \"\"\"\n",
    "    q_emb = encoder.encode(\n",
    "        [query], convert_to_numpy=True, normalize_embeddings=True\n",
    "    )\n",
    "    scores, indices = index.search(q_emb, k)\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append({\n",
    "            \"chunk\":  corpus[idx],\n",
    "            \"source\": metadata[idx][\"source\"],\n",
    "            \"score\":  float(score)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def show_retrieved(hits):\n",
    "    for i, h in enumerate(hits):\n",
    "        print(f\"\\n--- Chunk {i+1} | source: {h['source']} | score: {h['score']:.3f} ---\")\n",
    "        print(h[\"chunk\"][:400], \"...\" if len(h[\"chunk\"]) > 400 else \"\")\n",
    "\n",
    "\n",
    "# doing here a sanity check\n",
    "test_hits = retrieve(\"neurosymbolic drug discovery\")\n",
    "print(f\"Retrieved {len(test_hits)} chunks for sanity check:\")\n",
    "for h in test_hits:\n",
    "    print(f\"  [{h['score']:.3f}] {h['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b719aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed3bab3bd514945872afa18e20d4337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loaded: Qwen/Qwen2.5-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Generator LLM\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEN_MODEL,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=MAX_NEW_TOK,\n",
    "    do_sample=False,      # greedy for reproducibility in class demo\n",
    "    temperature=None,\n",
    "    top_p=None\n",
    ")\n",
    "print(f\"Generator loaded: {GEN_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c3b2d",
   "metadata": {},
   "source": [
    "**RAG inference**\n",
    "\n",
    "Two functions:\n",
    "- `llm_only(query)`: parametric generation, no retrieval\n",
    "- `rag_answer(query)`: retrieve then generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f530ef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions defined: llm_only(), rag_answer()\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PLAIN = (\n",
    "    \"You are a knowledgeable AI research assistant. \"\n",
    "    \"Answer the question as accurately as you can.\"\n",
    ")\n",
    "\n",
    "SYSTEM_RAG = (\n",
    "    \"You are a precise research assistant. \"\n",
    "    \"Answer the question using ONLY the provided context. \"\n",
    "    \"If the answer is not in the context, say: 'Not found in the provided documents.' \"\n",
    "    \"Always state which paper (source) supports your answer.\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_prompt(system: str, context: str, query: str) -> str:\n",
    "    \"\"\"Build a chat-style prompt compatible with Qwen2.5 / Mistral / SmolLM2.\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system}]\n",
    "    user_content = f\"{query}\" if not context else (\n",
    "        f\"Context from research papers:\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "    )\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_answer(raw_output: str, prompt: str) -> str:\n",
    "    \"\"\"Strip the prompt prefix from generator output.\"\"\"\n",
    "    return raw_output[len(prompt):].strip()\n",
    "\n",
    "\n",
    "def llm_only(query: str) -> str:\n",
    "    \"\"\"Pure parametric generation — no retrieval.\"\"\"\n",
    "    prompt = build_prompt(SYSTEM_PLAIN, \"\", query)\n",
    "    raw    = generator(prompt)[0][\"generated_text\"]\n",
    "    return extract_answer(raw, prompt)\n",
    "\n",
    "\n",
    "def rag_answer(query: str, k: int = TOP_K):\n",
    "    \"\"\"Retrieve top-k chunks, then generate a grounded answer.\"\"\"\n",
    "    hits    = retrieve(query, k)\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[Source: {h['source']} | relevance: {h['score']:.3f}]\\n{h['chunk']}\"\n",
    "        for h in hits\n",
    "    )\n",
    "    prompt = build_prompt(SYSTEM_RAG, context, query)\n",
    "    raw    = generator(prompt)[0][\"generated_text\"]\n",
    "    return extract_answer(raw, prompt), hits\n",
    "\n",
    "\n",
    "print(\"Functions defined: llm_only(), rag_answer()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52387a",
   "metadata": {},
   "source": [
    "**Test: LLM-only vs RAG-augmented**\n",
    "\n",
    "For each query we show:\n",
    "1. The LLM-only answer (parametric; may hallucinate)\n",
    "2. The retrieved chunks (with source filenames and similarity scores)\n",
    "3. The RAG-augmented answer (grounded; attributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd465b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUERY: What symbolic representation is used for molecules in Dash et al.'s neurosymbolic framework?\n",
      "================================================================================\n",
      "\n",
      "[LLM-only answer]\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but there seems to be an error in your question as no specific paper titled \"Dash et al.\" with a neurosymbolic framework specifically about molecular representations has been widely recognized in the literature up to my last update in October 2023.\n",
      "\n",
      "However, if you're referring to a neurosymbolic framework that deals with molecular representations, one common approach is to use chemical formulas or molecular structures as symbols. In computational models, these might be represented using SMILES (Simplified Molecular Input Line Entry System) or InChI (International Chemical Identifier) strings, which are standardized ways of encoding molecular structures.\n",
      "\n",
      "If you have more context or details about the specific framework you're referring to, I'd be happy to provide a more accurate answer based on that information.\n",
      "\n",
      "[Retrieved chunks]\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 | source: 2510.23379v1.pdf | score: 0.834 ---\n",
      "represented as a Grothendieck construction over an indexed family of partially-ordered sets. The construction yields a unified space linking symbolic and neural generation. • We implement and test SNGs on the real-world problem of generating potential in- hibitors for protein-targets. This constitutes the problem of ‘lead-discovery’, and re- alistic settings have the following charactestics. Given ...\n",
      "\n",
      "--- Chunk 2 | source: 2510.23379v1.pdf | score: 0.832 ---\n",
      "the implementation – instances generated – is also true of the specification (satisfies the constraints of the symbolic model). Unlike with classic formal methods, in SNG ML techniques play a role in obtaining both the specification and the implementation. We have proposed using the mathematical structure of fibered-posets by way of specifying the codomain of an SNG system. The paired elements com ...\n",
      "\n",
      "--- Chunk 3 | source: 2510.23379v1.pdf | score: 0.825 ---\n",
      "distribution. The set X is the set of instances generated that are in S. We see the paper contributing in the following ways to research into neurosymbolic systems: • We identify a class of hybrid neurosymbolic systems – called Symbolic Neural Gener- ators, or SNGs – which draws on the strengths of symbolic learning and neural-based generative models. Potential symbolic descriptions are examined,  ...\n",
      "\n",
      "--- Chunk 4 | source: 2510.23379v1.pdf | score: 0.809 ---\n",
      "rests on the observation that “the activity of any neu- ron can be represented as a proposition”. Thus arises an entire class of systems comprising a purely connectionist approach that approximate the patterns of categoric inference arising when using (often propositional, but not always) logical representations, as well as the pat- terns of plausible inference that arise when using probabilistic  ...\n",
      "\n",
      "[RAG answer]\n",
      "----------------------------------------\n",
      "Not found in the provided documents.\n",
      "\n",
      "================================================================================\n",
      "QUERY: Which datasets are used for drug-likeness or bioactivity prediction?\n",
      "================================================================================\n",
      "\n",
      "[LLM-only answer]\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several popular datasets used in the field of drug-likeness and bioactivity prediction. Here are some key ones:\n",
      "\n",
      "1. PubChem BioAssay Datasets: These include a variety of bioactivity data from various assays, which can be used to predict drug-like properties.\n",
      "\n",
      "2. ZINC Database: This is one of the most widely used databases for virtual screening and drug discovery. It contains over 1.3 billion commercially available small molecules.\n",
      "\n",
      "3. Tox21 Datasets: These datasets come from the National Institute of Environmental Health Sciences (NIEHS) and the National Toxicology Program (NTP), providing a wide range of chemical structures and their biological activity.\n",
      "\n",
      "4. ChEMBL: A large repository of bioactive compounds and their associated experimental data, including drug-like properties.\n",
      "\n",
      "5. BindingDB: Contains experimentally determined protein-ligand binding affinities, which can be useful for predicting drug-like properties.\n",
      "\n",
      "6. OpenTox: An open-source platform that provides a collection of bioactivity datasets, including those from the European Chemical Bureau (ECB).\n",
      "\n",
      "7. ChEMBL DrugBank: Combines information from ChEMBL with DrugBank, offering a comprehensive view of drug-like properties and bioactivities.\n",
      "\n",
      "8. KEGG Ligand Databases: While primarily focused on metabolic pathways, these databases contain a wealth of chemical structures that can be used for drug-likeness prediction.\n",
      "\n",
      "These datasets serve as valuable resources for researchers working on developing models for predicting drug-likeness and bioactivity. However, it's important to note that different studies may use different subsets or combinations of these datasets based on their specific needs and objectives.\n",
      "\n",
      "[Retrieved chunks]\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 | source: s10994-021-06090-8.pdf | score: 0.819 ---\n",
      "to GNNs (Dash et al., 2021c). 600 Machine Learning (2022) 111:575–623 1 3 5.2 \u0007Materials 5.2.1 \u0007Data For the empirical evaluation of our proposed BotGNNs, we use 73 benchmark datasets arising in the field of drug-discovery. Each dataset represents extensive drug evaluation effort at the NCI14 to experimentally determine the effectiveness of anti-cancer activity of a compound against a number of ce ...\n",
      "\n",
      "--- Chunk 2 | source: PhD_Thesis_Final.pdf | score: 0.784 ---\n",
      "a number of cell- lines [MOHU03]. These datasets correspond to the concentration parameter GI50, which is the concentration that results in 50% growth inhibition of tumour cells. Some of the datasets have been used in various data mining studies, such as in a study involving the use of graph kernels in machine learning [RSSB05]. These datasets are also used in the study of LRNNs [SAZ+18, ˇSou20].  ...\n",
      "\n",
      "--- Chunk 3 | source: 2021.07.09.451519v2.full.pdf | score: 0.775 ---\n",
      "the set of acceptable molecules generated in the sample of M molecules (acceptable molecules satisfy molecular constraints deﬁned on molecular properties). Act denotes the proportion of M ′ that are predicted active (the proxy model predicts an pIC50 ≥6.0); Sim denotes the proportion of M ′ that are similar to active target inhibitors (Tanimoto similarity to active JAK2 inhibitors > 0.75). The num ...\n",
      "\n",
      "--- Chunk 4 | source: s10994-021-05966-z.pdf | score: 0.774 ---\n",
      "(“propositionalisation”). Here, the clauses are not used in either of these roles, but as relations that augment the prior knowledge available to the GNN. 1618 Machine Learning (2021) 110:1609–1636 1 3 1. How do VEGNNs perform against standard GNNs? This compares GNNs with and without the inclusion of domain-knowledge. 2. Can the performance of VEGNNs be improved by using symbolic learner with acc ...\n",
      "\n",
      "[RAG answer]\n",
      "----------------------------------------\n",
      "Not found in the provided documents.\n",
      "\n",
      "================================================================================\n",
      "QUERY: How is background knowledge encoded in the ILP-based drug discovery approach?\n",
      "================================================================================\n",
      "\n",
      "[LLM-only answer]\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In an Inductive Logic Programming (ILP)-based drug discovery approach, background knowledge is typically encoded through logical rules or constraints that guide the learning process. Here's how it is usually implemented:\n",
      "\n",
      "1. **Rule Encoding**: Background knowledge is first formalized into logical rules or constraints. These rules often represent biological knowledge, chemical properties, and disease mechanisms relevant to the drug discovery problem. For example, a rule might state that \"a compound X interacts with protein Y if it has a certain binding affinity.\"\n",
      "\n",
      "2. **Knowledge Base Construction**: The encoded rules are compiled into a knowledge base. This knowledge base serves as the training data for the ILP system, providing context and prior information about what constitutes a valid solution.\n",
      "\n",
      "3. **Learning Algorithm**: An ILP algorithm then processes this knowledge base to learn patterns and relationships that generalize from the provided examples to new instances. The algorithm searches for hypotheses that best fit both the given examples and the background knowledge.\n",
      "\n",
      "4. **Constraint Satisfaction**: During the learning process, the ILP system must satisfy not only the positive examples but also the constraints imposed by the background knowledge. This ensures that the learned hypotheses are consistent with existing biological and chemical understanding.\n",
      "\n",
      "5. **Hypothesis Generation**: The system generates candidate hypotheses based on the available background knowledge. These hypotheses are evaluated against the training data to ensure they are both accurate and consistent with the provided knowledge.\n",
      "\n",
      "6. **Refinement and Iteration**: The learned hypotheses are refined through iterative refinement processes, where the system may modify or add to the hypotheses to better align with the background knowledge and the data.\n",
      "\n",
      "By encoding background knowledge in this manner, the ILP-based drug discovery approach aims to leverage existing scientific knowledge to guide the search for novel drug candidates, potentially\n",
      "\n",
      "[Retrieved chunks]\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 | source: PhD_Thesis_Final.pdf | score: 0.812 ---\n",
      "novel molecules for early-stage drug design. We show how a graph-based neural network with domain-knowledge is one component in a modular system design. Specifically, our conclusions from this chapter are as follows: (1) We have constructed a complete end- to-end neural-symbolic system that is capable of generating active molecules that may not be in any existing database; (2) We have demonstrated ...\n",
      "\n",
      "--- Chunk 2 | source: 2021.07.09.451519v2.full.pdf | score: 0.810 ---\n",
      "Using Domain-Knowledge to Assist Lead Discovery in Early-Stage Drug Design Tirtharaj Dash1, Ashwin Srinivasan1, Lovekesh Vig2, and Arijit Roy3 1 Dept. of CSIS & APPCAIR, BITS Pilani, Goa Campus, India 2 TCS Research, New Delhi, India 3 TCS Innovation Labs (Life Sciences Division), Hyderabad, India Abstract. We are interested in generating new small molecules which could act as inhibitors of a biol ...\n",
      "\n",
      "--- Chunk 3 | source: 27751-Article Text-31805-1-2-20240324.pdf | score: 0.800 ---\n",
      "early-stage drug design (see Fig. 1(a) showing the use of a Robot Scientist) as virtual screening, identifying qualita- tive and quantitative structure-activity relations (SARs) and so on. The broader picture is of a semi-automated scientific discovery pipeline involving feedback from from computa- tional chemists, synthetic chemists, and biologists and man- ufacturers, using results from simulati ...\n",
      "\n",
      "--- Chunk 4 | source: PhD_Thesis_Final.pdf | score: 0.797 ---\n",
      "2018. [SKTW17] Marwin H.S. Segler, Thierry Kogej, Christian Tyrchan, and Mark P. Waller. Generating focused molecule libraries for drug discovery with re- current neural networks. ACS Central Science, 4(1):120–131, 2017. [SLM20] Mattia Silvestri, Michele Lombardi, and Michela Milano. Injecting domain knowledge in neural networks: a controlled experiment on a constrained problem. arXiv preprint arX ...\n",
      "\n",
      "[RAG answer]\n",
      "----------------------------------------\n",
      "According to the provided context from \"PhD_Thesis_Final.pdf\" (relevance: 0.812), background knowledge is encoded in the ILP-based drug discovery approach by utilizing Inductive Logic Programming (ILP) techniques. Specifically, it mentions that the problem involves drawing instances from the conditional distribution of molecules given both domain-specific knowledge (B) and activity data (Y), DX|Y,B. The design consists of generators to approximate these distributions, incorporating symbolic domain-knowledge to assist in exploring the vast space of possible molecules.\n",
      "\n",
      "================================================================================\n",
      "QUERY: What are the main results comparing the neurosymbolic model to GNN baselines?\n",
      "================================================================================\n",
      "\n",
      "[LLM-only answer]\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but there is no widely recognized or publicly available study that directly compares a \"neurosymbolic model\" to Graph Neural Network (GNN) baselines in terms of specific results. The term \"neurosymbolic model\" is not commonly used in the context of neural networks and graph learning, and it's unclear what kind of model this refers to.\n",
      "\n",
      "Neurosymbolic models typically refer to approaches that combine neural networks with symbolic reasoning, which is quite different from GNNs. GNNs are specifically designed for tasks involving graph-structured data.\n",
      "\n",
      "If you're looking for comparisons between GNNs and other types of neural network models, such as feedforward neural networks or recurrent neural networks, those comparisons are well-documented in the literature. However, these comparisons would not be directly comparable to a neurosymbolic model.\n",
      "\n",
      "To get accurate results on how a neurosymbolic model performs compared to GNNs, you would need to:\n",
      "\n",
      "1. Identify a specific neurosymbolic model.\n",
      "2. Define clear benchmarks or tasks where both models can be evaluated.\n",
      "3. Conduct experiments and report the results.\n",
      "\n",
      "Without more specific information about the neurosymbolic model in question, I cannot provide detailed results. If you have more details about the neurosymbolic model you're referring to, I'd be happy to help you find relevant information or conduct a hypothetical comparison based on common practices.\n",
      "\n",
      "[Retrieved chunks]\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 | source: PhD_Thesis_Final.pdf | score: 0.824 ---\n",
      "for all variants of GNN. We examine the results in more detail: From Figure 4.3, it is evident that the per- formance of graph-based networks improves with the inclusion of domain-knowledge. A quantitative tabulation of wins, losses and draws is in Figure 4.4. These results again provide sufficient grounds to answer positively the primary research question addressed in this dissertation, namely: d ...\n",
      "\n",
      "--- Chunk 2 | source: PhD_Thesis_Final.pdf | score: 0.797 ---\n",
      "for training the VEGNNs (V EGNN1,...,5). The learning rate is 0.0005, weight decay parameter is 0.0001, the momentum factors are set to the default values of (β1, β2) = (0.9, 0.999). • The maximum number of training epochs is 1000. The batch size is 128. • We use an early-stopping mechanism [Pre98] to avoid overfitting during training. The resulting model is then saved and can be used for evaluati ...\n",
      "\n",
      "--- Chunk 3 | source: PhD_Thesis_Final.pdf | score: 0.794 ---\n",
      "visual clarity. Some Additional Comparisons Although not of direct relevance to the primary research conjecture of this dissertation (that is, the inclusion of domain-knowledge can significantly improve the performance of DNNs), it is nevertheless useful to ask how the two approaches we have investigated so far compare against each other. To answer this question, we perform a quantitative comparis ...\n",
      "\n",
      "--- Chunk 4 | source: PhD_Thesis_Final.pdf | score: 0.789 ---\n",
      "question of practical interest: Are BotGNNs better than V EGNNs that are studied in the preceding chapter? To answer this question, we provide some comparisons BotGNNs against V EGNNs for the same set of datasets and domain- knowledge. The methodology adopted for this comparison is similar to what was done for comparing BotGNNs against GNNs (refer subsection 5.3.3). Figure 5.7 provides a tabulatio ...\n",
      "\n",
      "[RAG answer]\n",
      "----------------------------------------\n",
      "The main results comparing the neurosymbolic model (VEGNNs) to GNN baselines (GNN variants without access to domain-knowledge) indicate significant improvements in predictive accuracy when domain-knowledge is incorporated. Specifically, the qualitative comparison in Figure 4.3 shows that the performance of graph-based networks improves with the inclusion of domain-knowledge. Quantitatively, these improvements are summarized in Figure 4.4, which presents a tabulation of wins, losses, and draws. This table indicates that across different GNN variants, the VEGNN models outperform their corresponding GNN counterparts, suggesting that the incorporation of domain-knowledge through vertex-enrichment enhances predictive accuracy.\n",
      "\n",
      "================================================================================\n",
      "QUERY: What deep learning models Dash has worked on?\n",
      "================================================================================\n",
      "\n",
      "[LLM-only answer]\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but there seems to be some confusion. There is no widely known entity called \"Dash\" that has been involved in deep learning model development. It's possible you might be thinking of Dash (company), which is a financial technology company, or Dash (software), which is a decentralized cryptocurrency. However, neither of these entities is typically associated with deep learning model development.\n",
      "\n",
      "If you meant to ask about a specific individual named Dash who works on deep learning models, I would need more context to provide accurate information. If you could clarify the name or provide additional details, I'd be happy to assist further.\n",
      "\n",
      "[Retrieved chunks]\n",
      "----------------------------------------\n",
      "\n",
      "--- Chunk 1 | source: PhD_Thesis_Final.pdf | score: 0.742 ---\n",
      "machines”, International Con- ference on Artificial Neural Networks, 2019. URL: https://doi.org/10.1007/978-3-030-30484-3_3 6. T. Dash, A. Srinivasan, L. Vig, O.I. Orhobor, R.D. King, “Large-scale assessment of deep relational machines”, International Conference on Inductive Logic Program- ming, 2018. URL: https://doi.org/10.1007/978-3-319-99960-9_2 (⋆Winner of the Best Student Paper Award) 187 Th ...\n",
      "\n",
      "--- Chunk 2 | source: s41598-021-04590-0.pdf | score: 0.717 ---\n",
      "networks on a wide variety of tasks makes a substantial case for their use in model construction, it is not immediately obvious how either (1) or (2) should be done with deep neural networks. In this paper, we examine ways of achieving (1), that is, the techniques for constructing deep neural networks from data and domain-knowledge concerning the problem. Understanding models constructed by deep n ...\n",
      "\n",
      "--- Chunk 3 | source: s10994-023-06399-6.pdf | score: 0.715 ---\n",
      "(1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4), 303–314. https://​doi.​org/​10.​1007/​BF025​51274 Dash, T., Srinivasan, A., Vig, L., Orhobor, O. I., & King, R. D. (2018). Large-scale assessment of deep relational machines. In International conference on inductive logic programming (pp 22–37). Springer. Dash, T., Srinivasan, A., Jo ...\n",
      "\n",
      "--- Chunk 4 | source: s10994-021-06090-8.pdf | score: 0.715 ---\n",
      "as the first of 3 Grand Challenges for ML and AI: “ML and AI are generally domain-agnostic ...Off-the-shelf practice treats [each of these] datasets in the same way and ignores domain knowledge that extends far beyond the raw data itself—such as physical laws, available forward simulations, and established invariances and symmetries—that is readily available ...Improving our ability to systematica ...\n",
      "\n",
      "[RAG answer]\n",
      "----------------------------------------\n",
      "Based on the information provided in the context, Thomas Dash has worked on the following deep learning models:\n",
      "\n",
      "1. Deep Relational Machines\n",
      "2. Graph Neural Networks (GNNs)\n",
      "3. Mode-Directed Inverse Entailment for GNNs\n",
      "\n",
      "These models are referenced through various publications listed in the context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEMO_QUERIES = [\n",
    "    \"What symbolic representation is used for molecules in Dash et al.'s neurosymbolic framework?\",\n",
    "    \"How is background knowledge encoded in the ILP-based drug discovery approach?\",\n",
    "    \"What deep learning models Dash has worked on?\"\n",
    "]\n",
    "\n",
    "SEP = \"=\" * 80\n",
    "\n",
    "for query in DEMO_QUERIES:\n",
    "    print(SEP)\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(SEP)\n",
    "\n",
    "    print(\"\\n[LLM-only answer]\")\n",
    "    print(\"-\" * 40)\n",
    "    ans_plain = llm_only(query)\n",
    "    print(ans_plain)\n",
    "\n",
    "    print(\"\\n[Retrieved chunks]\")\n",
    "    print(\"-\" * 40)\n",
    "    ans_rag, hits = rag_answer(query)\n",
    "    show_retrieved(hits)\n",
    "\n",
    "    print(\"\\n[RAG answer]\")\n",
    "    print(\"-\" * 40)\n",
    "    print(ans_rag)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e57434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question:  What is neurosymbolic learning?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[LLM-only]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurosymbolic learning is an emerging field that combines elements of neural networks (neuro) and symbolic AI (symbolic). This approach aims to leverage the strengths of both paradigms to create more robust, interpretable, and versatile AI systems.\n",
      "\n",
      "Key aspects of neurosymbolic learning include:\n",
      "\n",
      "1. Neural Networks: These provide the ability to learn complex patterns and relationships from data, which can be particularly useful for tasks where large amounts of data are available.\n",
      "\n",
      "2. Symbolic AI: This involves using logical rules, knowledge bases, and other formal methods to represent and reason about the world. It provides a way to ensure that AI systems can make sense of their environment and act in ways that align with human understanding and values.\n",
      "\n",
      "3. Integration: Neurosymbolic learning seeks to integrate these two approaches, allowing neural networks to handle the non-symbolic, data-driven aspects of learning while symbolic reasoning handles the symbolic, rule-based aspects.\n",
      "\n",
      "4. Interpretable Models: By combining neural networks with symbolic reasoning, these systems can often produce models that are more transparent and easier for humans to understand, which is crucial for applications like healthcare, finance, and autonomous vehicles.\n",
      "\n",
      "5. Hybrid Systems: These systems can switch between different modes of operation depending on the task at hand, potentially improving performance across various domains.\n",
      "\n",
      "6. Challenges: Integrating these two very different types of AI systems is not straightforward and requires significant research to overcome issues such as compatibility, efficiency, and scalability.\n",
      "\n",
      "Some notable examples of neurosymbolic learning systems include:\n",
      "\n",
      "- Neural-symbolic regression\n",
      "- Neural-symbolic classifiers\n",
      "- Neural-symbolic planners\n",
      "\n",
      "The field is still evolving, and many researchers are actively working on developing better methods and tools for neurosymbolic learning. As this\n",
      "\n",
      "[Retrieved chunks]\n",
      "\n",
      "--- Chunk 1 | source: 2510.23379v1.pdf | score: 0.757 ---\n",
      "rests on the observation that “the activity of any neu- ron can be represented as a proposition”. Thus arises an entire class of systems comprising a purely connectionist approach that approximate the patterns of categoric inference arising when using (often propositional, but not always) logical representations, as well as the pat- terns of plausible inference that arise when using probabilistic  ...\n",
      "\n",
      "--- Chunk 2 | source: 2510.23379v1.pdf | score: 0.734 ---\n",
      "distribution. The set X is the set of instances generated that are in S. We see the paper contributing in the following ways to research into neurosymbolic systems: • We identify a class of hybrid neurosymbolic systems – called Symbolic Neural Gener- ators, or SNGs – which draws on the strengths of symbolic learning and neural-based generative models. Potential symbolic descriptions are examined,  ...\n",
      "\n",
      "--- Chunk 3 | source: 2510.23379v1.pdf | score: 0.724 ---\n",
      "emergence of more abstract, mathematical perspectives on what represents a unified neurosymbolic system. Thus, Odense and d’Avila Garcez (2025) are concerned with identi- fying the conditions of semantic equivalence between architectures and representations; and De Smet and De Raedt (2025) provides the mathematical underpinning of a uniform infer- ence mechanism for reasoning in systems containing ...\n",
      "\n",
      "--- Chunk 4 | source: 2510.23379v1.pdf | score: 0.723 ---\n",
      "related area of statistical relational learning. In this paper, we are concerned with the alternative form of hybrid neurosymbolic systems that contains distinct neural and symbolic components. It is helpful to categorise hybrid systems based on the the principal function of each component in the system (Fig. 2).3 SNG as we propose it refers to a sub-class of systems in Category (B). We are concer ...\n",
      "\n",
      "[RAG answer]\n",
      "Neurosymbolic learning is a field that combines the strengths of both symbolic and connectionist approaches. It involves systems that can approximate patterns of categoric and plausible inference using logical and probabilistic representations respectively. This area has a substantial body of literature, including works like those by d'Avila Garcez et al. (2009), Besold et al. (2017), d'Avila Garcez and Lamb (2023), De Smet and De Raedt (2025), and Derkinderen et al. (2025). The context also mentions the development of more abstract, mathematical perspectives on what constitutes a unified neurosymbolic system, as seen in Odense and d'Avila Garcez (2025) and De Smet and De Raedt (2025).\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter your question: \").strip()\n",
    "if query:\n",
    "    print(SEP)\n",
    "    print(\"[LLM-only]\")\n",
    "    print(llm_only(query))\n",
    "\n",
    "    print(\"\\n[Retrieved chunks]\")\n",
    "    ans, hits = rag_answer(query)\n",
    "    show_retrieved(hits)\n",
    "\n",
    "    print(\"\\n[RAG answer]\")\n",
    "    print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829059c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers  : 9\n",
      "Total chunks  : 1094\n",
      "Avg chunks    : 121.6\n",
      "\n",
      "                                    paper  chunks\n",
      "                     PhD_Thesis_Final.pdf     456\n",
      "                   s10994-021-06090-8.pdf     135\n",
      "                   s10994-023-06399-6.pdf     117\n",
      "                         2510.23379v1.pdf     103\n",
      "                   s41598-021-04590-0.pdf      73\n",
      "                   s10994-021-05966-z.pdf      70\n",
      "             2021.07.09.451519v2.full.pdf      50\n",
      "                  978-3-030-30484-3_3.pdf      47\n",
      "27751-Article Text-31805-1-2-20240324.pdf      43\n"
     ]
    }
   ],
   "source": [
    "# a sanity checker\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "source_counts = Counter(m[\"source\"] for m in metadata)\n",
    "df = pd.DataFrame(source_counts.items(), columns=[\"paper\", \"chunks\"])\n",
    "df = df.sort_values(\"chunks\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total papers  : {len(df)}\")\n",
    "print(f\"Total chunks  : {df['chunks'].sum()}\")\n",
    "print(f\"Avg chunks    : {df['chunks'].mean():.1f}\")\n",
    "print()\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106484b1-6e26-45dd-b443-ee02889a6d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
